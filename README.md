# Webtools Interface Specification

## Introduction

This specification aims to describe a simple protocol for LLMs to discover and use remote APIs, while minimizing the context window used. While Anthropic's Model Context Protocol (MCP) is primarily used for the purpose, its design is non-optimal. MCP began as a simple STDIO‑based local solution but evolved into a complex, stateful HTTP/SSE system that burdens developers with session management and infrastructure headaches. Maintaining persistent connections conflicts with stateless microservice patterns, leading to scalability and load‑balancing challenges. This specification adopts a minimal, stateless design to avoid these pitfalls.

## Overview

Webtools expose a lightweight, HTTP‑based contract that allows consumers to

* **Discover** capabilities through self‑describing metadata
* **Validate** inputs and outputs via JSON Schema definitions
  * one schema for the request object (`requestSchema`)
  * one schema for the response object (`responseSchema`)
* **Execute** actions with optional per‑request configuration
* **Consume** predictable, strongly‑typed responses
* **Lock-in** specific API versions to improve security

## Use Case Scenario

Webtools are defined by URLs. The typical workflow follows these steps:

1. **Discovery**: A user finds a webtool URL from a tool provider, marketplace, or other source
2. **Metadata Retrieval**: The user's system issues a GET request to the URL to retrieve the webtool's metadata
3. **Configuration**: The user fills in configuration data according to the `configSchema` defined in the metadata
4. **Integration**: The system is now able to use the webtool with LLMs, passing the configuration and handling requests/responses

### Security Considerations

After reviewing the schemas and metadata, users may choose to lock-in a specific version by storing the validated metadata on their side and no longer fetching it from the remote server. This prevents potential security risks where malicious instructions could be injected into the LLM context through schema changes in newer versions of the webtool metadata.

## HTTP Methods

### GET / — Webtool Metadata (latest)

Returns metadata about the **latest** version of the webtool.

```json
{
  "name": "webtool_name",
  "description": "Human‑readable description of what this webtool does",
  "version": "2.1.0",
  "actions": [
    {
      "name": "action_name",
      "description": "What this action does",
      "requestSchema": { /* JSON Schema for request */ },
      "responseSchema": { /* JSON Schema for response */ }
    }
  ],
  "configSchema": { /* JSON Schema for configuration */ },
  "defaultConfig": { /* Default configuration values */ }
}
```

### GET /{version} — Webtool Metadata (specific version)

Returns metadata **for the specified semantic version**. Use this to fetch historical versions or pin a client to a stable release. The `version` parameter is optional; if omitted, the server SHOULD default to the latest version.

```http
GET /1.0.0
```

```json
{
  "name": "weather",
  "description": "Provides weather information",
  "version": "1.0.0",
  "actions": [ /* …as above… */ ],
  "configSchema": { /* … */ },
  "defaultConfig": { /* … */ }
}
```

> **Note**: If the version is not found, the endpoint should return `404 Not Found` with an error envelope identical to the standard error response.

### POST / — Webtool Execution

```json
{
  "sessionId": "unique-session-identifier",
  "version": "1.0.0",
  "action": "action_name",
  "config": { /* Optional configuration object matching configSchema */ },
  "request": { /* Required data matching requestSchema */ }
}
```

The `sessionId` field is optional and used for maintaining state across multiple requests to the same webtool. The `version` field is optional; if omitted, the server SHOULD default to the latest version. The `config` property contains data that is not generated by the LLM but rather supplied by the environment, allowing minimization of context use, passing security credentials, parameter defaults, or user-configured values.

#### Response Examples

##### Successful Response

```json
{
  "status": "ok",
  "data": { /* Action‑specific result */ }
}
```

##### Error Response

```json
{
  "status": "error",
  "error": {
    "code": "INVALID_INPUT",
    "message": "Validation failed for field 'location'"
  }
}
```

## Content Types

All requests and responses MUST use `application/json` content type. Servers MUST include `Content-Type: application/json` headers in their responses.

## JSON Schema Requirements

Webtools MAY use any JSON Schema features. Schemas SHOULD include descriptions and example values to help LLMs understand the expected data structure and format.

## Error Handling

The specification defines standard error codes for common validation errors:
- `WEBTOOL_NOT_FOUND` - Requested webtool or version does not exist
- `SCHEMA_ERROR` - Request data does not match the action's requestSchema
- `CONFIG_ERROR` - Configuration data does not match the webtool's configSchema
- `RATE_LIMIT` - Request rate limit exceeded
- `INTERNAL_ERROR` - Unrecoverable server error

Webtools MAY define their own custom error codes for domain-specific errors. Error messages SHOULD be human-readable.

## Integration Guides

### Using Webtools with the **Vercel AI SDK**

The Vercel AI SDK supports OpenAI‑style *tool calling* out‑of‑the‑box.

#### 1 – Fetch Metadata at Build Time

```ts
// lib/tools/weather.ts
import type { Tool } from "ai";

export async function getWeatherTool(): Promise<Tool> {
  const res = await fetch("/api/webtools/weather");
  const meta = await res.json();
  return {
    name: meta.name,
    description: meta.description,
    parameters: meta.actions[0].requestSchema, // <-- uses requestSchema
    execute: async (args) => {
      const exec = await fetch("/api/webtools/weather", {
        method: "POST",
        headers: { "content-type": "application/json" },
        body: JSON.stringify({
          sessionId: crypto.randomUUID(),
          action: args.action,
          request: args
        })
      });
      return (await exec.json()).data; // unwrap the envelope
    }
  };
}
```

#### 2 – Create a Tool Caller

```ts
import { createToolCaller } from "ai/tool-caller";
import OpenAI from "@ai-sdk/openai";
import { getWeatherTool } from "@/lib/tools/weather";

const toolCaller = createToolCaller([await getWeatherTool()]);

export async function chat(messages) {
  const llm = new OpenAI();
  const modelResponse = await llm.chat({ messages, tools: toolCaller.tools });
  const final = await toolCaller.call(modelResponse);
  return final;
}
```

> **Tip:** The AI SDK automatically translates `requestSchema` into the function‑calling format the model expects.

### Using Webtools with **LangChain**

LangChain's `StructuredTool` helper lets you wrap a webtool with schema metadata so agents can invoke it.

```python
from langchain_core.tools import StructuredTool
import requests, uuid

WEATHER_ENDPOINT = "https://api.example.com/webtools/weather"

def run_get_current(location: str, units: str = "metric"):
    body = {
        "sessionId": str(uuid.uuid4()),
        "action": "get_current",
        "request": {"location": location},
        "config": {"units": units}
    }
    return requests.post(WEATHER_ENDPOINT, json=body, timeout=10).json()

weather_tool = StructuredTool.from_function(
    func=run_get_current,
    name="get_current_weather",
    description="Return the current weather for a given location via the Weather webtool",
    schema={
        "type": "object",
        "properties": {
            "location": {"type": "string"},
            "units": {"type": "string", "enum": ["metric", "imperial"], "default": "metric"}
        },
        "required": ["location"]
    }
)
```

Then add `weather_tool` to any LCEL runnable or agent:

```python
from langchain_openai import ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_functions_agent

llm = ChatOpenAI(model_name="gpt-4o")
agent = create_openai_functions_agent(llm, tools=[weather_tool])
executor = AgentExecutor(agent=agent, tools=[weather_tool])

result = executor.invoke("Should I take an umbrella to Paris today?")
print(result)
```

## Error Handling & Best Practices

* Validate client input against each action's `requestSchema` before issuing a POST.
* For recoverable failures (4xx), return `status: "error"` with appropriate error codes.
* For unrecoverable server errors (5xx), set code `INTERNAL_ERROR` and avoid leaking internals.
* Use standard HTTP status codes alongside the JSON response for broad compatibility.
* Rate limiting, quotas, and other implementation details are left to implementers.

## Authentication & Authorization

This specification is agnostic regarding authorization schemes. Consumers MAY include an Authorization: Bearer <token> header on every request. Additional details—such as token scopes, expiration, or refresh flows—are considered implementation-specific and are not mandated by this spec.

---

**Version:** 2025‑06‑30
